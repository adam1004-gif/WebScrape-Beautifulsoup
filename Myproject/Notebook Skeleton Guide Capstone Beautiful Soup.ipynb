{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Webscrapping using BeautifulSoup\n",
    "\n",
    "This notebook contains guidances & tasks on the data processing for the application\n",
    "\n",
    "(Please insert the background here )\n",
    "\n",
    "\n",
    "## Requesting the Data and Creating a BeautifulSoup\n",
    "\n",
    "Let's begin with requesting the web from the site with `get` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T05:12:23.275508Z",
     "start_time": "2020-01-13T05:12:20.009898Z"
    }
   },
   "outputs": [],
   "source": [
    "from flask import Flask, render_template\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "\n",
    "#don't change this\n",
    "matplotlib.use('Agg')\n",
    "app = Flask(__name__) #do not change this\n",
    "\n",
    "#insert the scrapping here\n",
    "url_get = requests.get(\"https://www.imdb.com/search/title/?release_date=2019-01-01,2019-12-31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize what exactly you get from the `request.get`, we can use .content so ee what we exactly get, in here i slice it so it won't make our screen full of the html we get from the page. You can delete the slicing if you want to see what we fully get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T05:12:23.290648Z",
     "start_time": "2020-01-13T05:12:23.277650Z"
    }
   },
   "outputs": [],
   "source": [
    "url_get.content[1:777]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we get a very unstructured and complex html, which actually contains the codes needed to show the webpages on your web browser. But we as human still confused what and where we can use that piece of code, so here where we use the beautifulsoup. Beautiful soup class will result a beautifulsoup object. Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. \n",
    "\n",
    "Let's make Beautiful soup object and feel free to explore the object here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T05:12:23.808122Z",
     "start_time": "2020-01-13T05:12:23.292610Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(url_get.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the right key to scrap the data & Extracting the right information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the key and put the key into the `.find()` Put all the exploring the right key at this cell. (please change this markdown with your explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T05:12:23.878904Z",
     "start_time": "2020-01-13T05:12:23.854974Z"
    }
   },
   "outputs": [],
   "source": [
    "div = soup.find(\"div\", attrs={\"class\",\"lister-list\"})\n",
    "print(div.prettify()[1:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the scrapping process here (please change this markdown with your explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T05:12:24.008256Z",
     "start_time": "2020-01-13T05:12:23.980358Z"
    }
   },
   "outputs": [],
   "source": [
    "temp = [] #initiating a tuple\n",
    "\n",
    "for i in range (50):\n",
    "#insert the scrapping process here\n",
    "    \n",
    "    tab = div.find_all(\"h3\")[i]\n",
    "    \n",
    "    judul = tab.find_all(\"a\")[0].text\n",
    "    judul = judul.strip()\n",
    "\n",
    "    rating = div.find_all(\"strong\")[i].text\n",
    "\n",
    "    meta = soup.select(\".ratings-bar , .ratings-metascore\")\n",
    "    meta = [title.text for title in meta]\n",
    "    meta = meta[i].strip()\n",
    "    meta = meta.replace(meta[:59],\"\")\n",
    "    meta = meta[6:8]\n",
    "\n",
    "\n",
    "    votes = soup.select(\".sort-num_votes-visible span:nth-child(2)\")\n",
    "    votes = [title.text for title in votes]\n",
    "    votes = votes[i].replace(\",\",\"\")\n",
    "\n",
    "    temp.append((judul,rating,meta,votes))\n",
    "\n",
    "\n",
    "temp = temp[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data frame & Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the array into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T05:12:41.517372Z",
     "start_time": "2020-01-13T05:12:29.130015Z"
    }
   },
   "outputs": [],
   "source": [
    "#change into dataframe\n",
    "data = pd.DataFrame(temp, columns = ('Judul',\"IMDB Rating\",\"Metascore\",\"Votes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the data cleaning here (please change this markdown with your explanation of what you do for data wrangling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T05:12:59.165559Z",
     "start_time": "2020-01-13T05:12:58.910012Z"
    }
   },
   "outputs": [],
   "source": [
    "#insert data wrangling here\n",
    "data[\"Judul\"] = data[\"Judul\"].astype(\"category\")\n",
    "data[\"IMDB Rating\"] = data[\"IMDB Rating\"].astype(\"float64\")\n",
    "data[\"Metascore\"] = data[\"Metascore\"].replace(\"\",0)\n",
    "data[\"Metascore\"] = data[\"Metascore\"].astype(\"float64\")\n",
    "data[\"Votes\"] = data[\"Votes\"].astype(\"float64\")\n",
    "\n",
    "top_7 = data.sort_values(['IMDB Rating','Votes'], ascending=False).head(7)\n",
    "top_7 = top_7[::-1]\n",
    "#end of data wranggling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing your webscrapping to the flask dashboard\n",
    "\n",
    "- Copy paste all of your web scrapping process to the desired position on the `app.py`\n",
    "- Changing the title of the dasboard at `index.html`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing This Notebook with Your Analysis and Conclusion\n",
    "\n",
    "First you can do start with making the data visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T05:20:56.208237Z",
     "start_time": "2020-01-13T05:20:56.076043Z"
    }
   },
   "outputs": [],
   "source": [
    "Berdasarkan data yang divisualisasikan terdapat 7 rekomendasi film terbaik yang disortir \n",
    "berdasarkan Rating IMDB dan Jumlah Votes. Dari 7 rekomendasi movie tersebut adalah\n",
    "    1) Chernobyl\n",
    "    2) The Mandalorian\n",
    "    3) The Boys\n",
    "    4) Kimetsu No Yaiba\n",
    "    5) Akusoku No Neverland\n",
    "    6) Gisaengchung\n",
    "    7) Joker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(Put your analysis and conclusion here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Challange\n",
    "\n",
    "This will be not included to the scoring. \n",
    "\n",
    "- You can create additional analysis from the data.\n",
    "- Implement it to the dashboard with at `app.py` dan `index.html`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone1",
   "language": "python",
   "name": "capstone1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
